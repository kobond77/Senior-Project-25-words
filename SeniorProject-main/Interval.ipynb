{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "import time\n",
    "from HandTrackerRenderer import HandTrackerRenderer\n",
    "from HandTrackerEdge import HandTracker\n",
    "from FPS import FPS, now\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labelling  images\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing variables\n",
    "# Path for exported data, numpy arrays\n",
    "DATA_PATH = os.path.join('Collected_Data') \n",
    "# Actions that we try to detect\n",
    "actions = np.array(['a','am','are','do','for','have','Hello','I','is','it','pretty','sell','shirt'\n",
    "                    ,'skirt','so','the','to','wear','you','one','two','three','four','five','six'\n",
    "                    ,'about','bad','look','think'])\n",
    "\n",
    "# Thirty videos worth of data\n",
    "no_sequences =60\n",
    "\n",
    "# Videos are going to be 30 frames in length\n",
    "sequence_length = 30\n",
    "\n",
    "# Folder start\n",
    "start_folder = 30\n",
    "\n",
    "label_map = {label:num for num, label in enumerate(actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences, labels = [], []\n",
    "for action in actions:\n",
    "    for sequence in range(no_sequences):\n",
    "        res = np.load(os.path.join(DATA_PATH, action, str(action)+'_'+ str(sequence)+'.npy'))\n",
    "        sequences.append(res)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sign_model = keras.models.load_model(\"action19_LSTM_New.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = [(245,117,16), (117,245,16), (16,117,245)]\n",
    "def prob_viz(res, actions, input_frame, colors):\n",
    "    output_frame = input_frame.copy()\n",
    "    for num, prob in enumerate(res):\n",
    "        cv2.rectangle(output_frame, (0,60+num*40), (int(prob*100), 90+num*40), colors[num], -1)\n",
    "        cv2.putText(output_frame, actions[num], (0, 85+num*40), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    return output_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractData(hands) :\n",
    "    rh_data = np.zeros((21, 3))\n",
    "    lh_data = np.zeros((21, 3))\n",
    "    for hand in hands : \n",
    "        Label = hand.label\n",
    "        if Label == 'left' : \n",
    "            lh_data = hand.norm_landmarks \n",
    "        elif Label == 'right' : \n",
    "            rh_data = hand.norm_landmarks\n",
    "    return lh_data,rh_data\n",
    "\n",
    "def most_freq(List) :\n",
    "    return max(set(List), key = List.count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palm detection blob     : C:\\Secret\\College\\Senior\\SIgnExactEnglish\\models\\palm_detection_sh4.blob\n",
      "Landmark blob           : C:\\Secret\\College\\Senior\\SIgnExactEnglish\\models\\hand_landmark_lite_sh4.blob\n",
      "PD post processing blob : C:\\Secret\\College\\Senior\\SIgnExactEnglish\\custom_models\\PDPostProcessing_top2_sh1.blob\n",
      "Sensor resolution: (1920, 1080)\n",
      "Internal camera FPS set to: 36\n",
      "Internal camera image size: 1152 x 648 - pad_h: 252\n",
      "Creating pipeline...\n",
      "Creating Color Camera...\n",
      "Creating Palm Detection pre processing image manip...\n",
      "Creating Palm Detection Neural Network...\n",
      "Creating Palm Detection post processing Neural Network...\n",
      "Creating Hand Landmark pre processing image manip...\n",
      "Creating Hand Landmark Neural Network (2 threads)...\n",
      "Pipeline created.\n",
      "Pipeline started - USB speed: SUPER\n"
     ]
    }
   ],
   "source": [
    "tracker = HandTracker(solo = False)\n",
    "\n",
    "renderer = HandTrackerRenderer(\n",
    "        tracker=tracker,\n",
    "        output=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1. New detection variables\n",
    "lh = []\n",
    "rh = []\n",
    "sentence = []\n",
    "pred = []\n",
    "threshold = 0.75\n",
    "word_threshold = 25\n",
    "now_sign=[]\n",
    "font=cv2.FONT_HERSHEY_SIMPLEX\n",
    "color=(0,255,0)\n",
    "size = 1\n",
    "thickness=2\n",
    "while True:\n",
    "    frame, hands, bag = tracker.next_frame()\n",
    "    if frame is None: break\n",
    "    # Draw hands\n",
    "    hand = renderer.draw(frame, hands, bag)\n",
    "    key = renderer.waitKey(delay=1)\n",
    "    lh_data,rh_data = extractData(hands)\n",
    "    lh.append(lh_data)\n",
    "    rh.append(rh_data)\n",
    "    lh_used=lh[-30:]\n",
    "    rh_used=rh[-30:]\n",
    "    lrh = np.concatenate([lh_used,rh_used])\n",
    "    lrh_used = lrh.reshape(lrh.shape[0],-1)\n",
    "    cv2.putText(frame,f\"data = {len(lrh)}\",(20,50),font,size,color,thickness)\n",
    "    # predictions logic\n",
    "    if len(lrh) == 60:\n",
    "        res = sign_model.predict(np.expand_dims(lrh_used, axis=0))[0]\n",
    "        predicted_sign = actions[np.argmax(res)]\n",
    "        confidence = res[np.argmax(res)]\n",
    "        if (confidence > threshold) :\n",
    "            pred.append(predicted_sign)\n",
    "        if len(pred) >= 30 : \n",
    "            if (most_freq(pred[-30:]) not in now_sign) & (pred.count(most_freq(pred[-30:])) >= word_threshold) :\n",
    "                target = most_freq(pred[-30:])\n",
    "                now_sign = [target]\n",
    "                print(target)\n",
    "            #print(predicted_sign)\n",
    "            #print(confidence)\n",
    "            #now_sign=[actions[np.argmax(res)]]\n",
    "        #lh = []\n",
    "        #rh = []\n",
    "        lh.pop(0)\n",
    "        rh.pop(0)\n",
    "        print(pred)\n",
    "    if key == 27 or key == ord('q'):\n",
    "        break\n",
    "renderer.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracker.exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
